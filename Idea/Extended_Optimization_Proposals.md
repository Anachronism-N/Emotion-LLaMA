# HERO æ‰©å±•ä¼˜åŒ–æ–¹æ¡ˆè¯¦è§£ (Extended Optimization Proposals)

**æ—¥æœŸ**: 2026-01-11
**ç‰ˆæœ¬**: 1.0
**çŠ¶æ€**: ç ”ç©¶ä¸è§„åˆ’ä¸­

æœ¬æ–‡æ¡£åŸºäº Phase 5 çš„åŸºç¡€å®ç°ï¼Œæå‡ºæ›´å¤šé«˜çº§ä¼˜åŒ–æ€è·¯åŠå…¶è¯¦ç»†å®ç°æ–¹æ¡ˆï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æå‡ HERO æ¡†æ¶çš„æ€§èƒ½ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚

---

## 1. å·²å®ç°ä¼˜åŒ–å›é¡¾

| åŠŸèƒ½ | æ–‡ä»¶ä½ç½® | çŠ¶æ€ |
| :--- | :--- | :--- |
| Evidence Imputation | `evidence_imputation.py` | âœ… å·²å®ç° |
| Temperature Scaling | `integration_layer.py` | âœ… å·²å®ç° |
| Pipeline é›†æˆ | `EvidenceIntegrationLayer.forward()` | âœ… å·²å®ç° |
| **MultiModal Contrastive Loss** | `hero_loss.py` | âœ… å·²å®ç° |
| **Modality Entropy Regularizer** | `hero_loss.py` | âœ… å·²å®ç° |
| **Interpretability Module** | `interpretability.py` | âœ… å·²å®ç° |
| **Smart Gradient Checkpointing** | `optimization_utils.py` | âœ… å·²å®ç° |
| **FlashAttention V2 / SDPA** | `optimization_utils.py` | âœ… å·²å®ç° |
| **QLoRA Setup** | `optimization_utils.py` | âœ… å·²å®ç° |
| **Mixed Precision (AMP)** | `optimization_utils.py` | âœ… å·²å®ç° |
| **torch.compile** | `optimization_utils.py` | âœ… å·²å®ç° |

---


## 2. æ‰©å±•ä¼˜åŒ–æ–¹æ¡ˆ (Extended Proposals)

### 2.1 Confidence-Aware Loss Weighting (ç½®ä¿¡åº¦æ„ŸçŸ¥æŸå¤±åŠ æƒ)

**é—®é¢˜**: å½“å‰æ‰€æœ‰æ ·æœ¬çš„ Loss æƒé‡ç›¸åŒï¼Œä½†æ¨¡å‹å¯¹æŸäº›æ ·æœ¬çš„é¢„æµ‹ç½®ä¿¡åº¦å¯èƒ½å·®å¼‚å·¨å¤§ã€‚

**æ–¹æ¡ˆ**:
```python
class ConfidenceAwareLoss(nn.Module):
    """
    æ ¹æ®æ¨¡å‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´ Loss æƒé‡ã€‚
    ä½ç½®ä¿¡åº¦æ ·æœ¬è·å¾—æ›´é«˜çš„æ¢¯åº¦æƒé‡ï¼Œä¿ƒè¿›å›°éš¾æ ·æœ¬å­¦ä¹ ã€‚
    """
    def __init__(self, base_loss: nn.Module, confidence_scale: float = 2.0):
        super().__init__()
        self.base_loss = base_loss
        self.confidence_scale = confidence_scale
        
    def forward(self, pred, target, confidence):
        # ä½ç½®ä¿¡åº¦ -> é«˜æƒé‡ (Focal Loss æ€æƒ³)
        weight = (1 - confidence) ** self.confidence_scale
        loss = self.base_loss(pred, target)
        return (loss * weight).mean()
```

**é›†æˆä½ç½®**: `hero_loss.py` æˆ– `train_hero.py`

**é¢„æœŸæ”¶ç›Š**: æå‡å›°éš¾æ ·æœ¬ï¼ˆå¦‚åè®½ã€å¾®è¡¨æƒ…ï¼‰çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚

---

### 2.2 Multi-Scale Temporal Fusion (å¤šå°ºåº¦æ—¶åºèåˆ)

**é—®é¢˜**: å½“å‰ Q-Former åªè¾“å‡ºå›ºå®šé•¿åº¦çš„ 32 ä¸ª Queryï¼Œå¯èƒ½ä¸¢å¤±ç»†ç²’åº¦æ—¶åºä¿¡æ¯ã€‚

**æ–¹æ¡ˆ**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Multi-Scale Temporal Pyramid              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Scale 1 (Coarse): Global Avg Pool -> [B, 1, D]     â”‚
â”‚  Scale 2 (Medium): Segment Pool (4æ®µ) -> [B, 4, D]  â”‚
â”‚  Scale 3 (Fine):   Q-Former (32 queries) -> [B,32,D]â”‚
â”‚                                                     â”‚
â”‚  Fusion: Concat + Cross-Attention -> [B, 37, D]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç éª¨æ¶**:
```python
class MultiScaleTemporalFusion(nn.Module):
    def __init__(self, hidden_dim, num_segments=4, num_queries=32):
        super().__init__()
        self.segment_pool = nn.AdaptiveAvgPool1d(num_segments)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.cross_attn = nn.MultiheadAttention(hidden_dim, 8, batch_first=True)
        
    def forward(self, seq_features):  # [B, Seq, D]
        # Scale 1: Global
        global_feat = self.global_pool(seq_features.transpose(1,2)).transpose(1,2)
        # Scale 2: Segment
        segment_feat = self.segment_pool(seq_features.transpose(1,2)).transpose(1,2)
        # Scale 3: Fine (assume already processed by Q-Former)
        fine_feat = ...
        
        # Hierarchical fusion
        multi_scale = torch.cat([global_feat, segment_feat, fine_feat], dim=1)
        fused, _ = self.cross_attn(multi_scale, multi_scale, multi_scale)
        return fused
```

**é›†æˆä½ç½®**: `observation_experts.py` -> `ModalityQFormer`

**é¢„æœŸæ”¶ç›Š**: åŒæ—¶æ•æ‰å…¨å±€è¶‹åŠ¿å’Œå±€éƒ¨ç»†èŠ‚ï¼ˆå¦‚å¾®è¡¨æƒ…ç¬é—´ï¼‰ã€‚

---

### 2.3 Contrastive Evidence Alignment (å¯¹æ¯”è¯æ®å¯¹é½)

**é—®é¢˜**: å½“å‰ SCCL Loss åªå¯¹é½ Visual-Textï¼Œä½†å¤šæ¨¡æ€é—´çš„å¯¹é½å¯ä»¥æ›´å…¨é¢ã€‚

**æ–¹æ¡ˆ**:
```python
class MultiModalContrastiveLoss(nn.Module):
    """
    åœ¨æ‰€æœ‰æ¨¡æ€å¯¹ä¹‹é—´æ‰§è¡Œå¯¹æ¯”å­¦ä¹ :
    - Audio-Text Alignment
    - Visual-Audio Alignment  
    - AU-Text Alignment (é¢éƒ¨è¡¨æƒ…ä¸æ–‡æœ¬æè¿°)
    """
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
        
    def forward(self, modal_features: Dict[str, torch.Tensor]):
        # modal_features: {'audio': [B,D], 'visual': [B,D], 'text': [B,D], ...}
        total_loss = 0
        pairs = [('audio', 'text'), ('visual', 'audio'), ('au', 'text')]
        
        for m1, m2 in pairs:
            if m1 in modal_features and m2 in modal_features:
                feat1 = F.normalize(modal_features[m1], dim=-1)
                feat2 = F.normalize(modal_features[m2], dim=-1)
                
                # InfoNCE Loss
                logits = torch.matmul(feat1, feat2.T) / self.temperature
                labels = torch.arange(feat1.size(0), device=feat1.device)
                loss = F.cross_entropy(logits, labels)
                total_loss += loss
                
        return total_loss / len(pairs)
```

**é›†æˆä½ç½®**: `hero_loss.py`

**é¢„æœŸæ”¶ç›Š**: æ„å»ºæ›´å¼ºçš„è·¨æ¨¡æ€ç‰¹å¾ç©ºé—´ï¼Œæå‡åè®½ç­‰å¤æ‚åœºæ™¯çš„è¯†åˆ«ã€‚

---

### 2.4 Modality Importance Regularization (æ¨¡æ€é‡è¦æ€§æ­£åˆ™åŒ–)

**é—®é¢˜**: `AdaptiveQueryGenerator` å¯èƒ½è¿‡åº¦ä¾èµ–æŸä¸€æ¨¡æ€ï¼Œå¯¼è‡´å…¶ä»–æ¨¡æ€ä¿¡æ¯è¢«å¿½ç•¥ã€‚

**æ–¹æ¡ˆ**:
```python
class ModalityEntropyRegularizer(nn.Module):
    """
    é¼“åŠ±æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒæ›´å‡åŒ€ï¼Œé¿å…å•ä¸€æ¨¡æ€ä¸»å¯¼ã€‚
    ä½¿ç”¨ç†µä½œä¸ºæ­£åˆ™åŒ–é¡¹: H(weights) = -sum(w * log(w))
    """
    def __init__(self, min_entropy_ratio=0.5):
        super().__init__()
        self.min_entropy_ratio = min_entropy_ratio
        
    def forward(self, attention_weights):  # [B, N_modalities]
        # Compute entropy
        log_weights = torch.log(attention_weights + 1e-8)
        entropy = -(attention_weights * log_weights).sum(dim=-1)  # [B]
        
        # Maximum possible entropy (uniform distribution)
        num_modalities = attention_weights.size(-1)
        max_entropy = torch.log(torch.tensor(num_modalities, dtype=torch.float))
        
        # Penalize if entropy is too low
        entropy_ratio = entropy / max_entropy
        penalty = F.relu(self.min_entropy_ratio - entropy_ratio)
        
        return penalty.mean()
```

**é›†æˆä½ç½®**: `hero_model.py` -> `forward()` ä¸­ä½œä¸ºè¾…åŠ© loss

**é¢„æœŸæ”¶ç›Š**: å¼ºåˆ¶æ¨¡å‹åˆ©ç”¨æ‰€æœ‰å¯ç”¨æ¨¡æ€ä¿¡æ¯ï¼Œæå‡é²æ£’æ€§ã€‚

---

### 2.5 Emotion-Aware Data Augmentation (æƒ…æ„Ÿæ„ŸçŸ¥æ•°æ®å¢å¼º)

**é—®é¢˜**: å¤šæ¨¡æ€æƒ…æ„Ÿæ•°æ®ç¨€ç¼ºï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚

**æ–¹æ¡ˆ**:
1. **Modality Mixup**: å°†ä¸¤ä¸ªæ ·æœ¬çš„æ¨¡æ€ç‰¹å¾æŒ‰æ¯”ä¾‹æ··åˆ
2. **Temporal Jittering**: å¯¹æ—¶åºç‰¹å¾è¿›è¡Œéšæœºåç§»
3. **Cross-Modal Dropout**: éšæœºä¸¢å¼ƒæŸä¸ªæ¨¡æ€çš„æŸäº›æ—¶é—´æ®µ

```python
class EmotionAwareAugmentor:
    def __init__(self, mixup_alpha=0.2, jitter_ratio=0.1):
        self.mixup_alpha = mixup_alpha
        self.jitter_ratio = jitter_ratio
        
    def mixup(self, features1, features2, labels1, labels2):
        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)
        mixed_features = lam * features1 + (1 - lam) * features2
        # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨è½¯æ ‡ç­¾
        mixed_labels = lam * labels1 + (1 - lam) * labels2
        return mixed_features, mixed_labels
        
    def temporal_jitter(self, seq_features, max_shift=3):
        shift = random.randint(-max_shift, max_shift)
        return torch.roll(seq_features, shifts=shift, dims=1)
```

**é›†æˆä½ç½®**: `hero_dataset.py` æˆ– DataLoader collate function

**é¢„æœŸæ”¶ç›Š**: å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚

---

### 2.6 Inference-Time Ensemble (æ¨ç†æ—¶é›†æˆ)

**é—®é¢˜**: å•ä¸€æ¨¡å‹å¯èƒ½åœ¨æŸäº›åœºæ™¯ä¸‹ä¸ç¨³å®šã€‚

**æ–¹æ¡ˆ**:
```python
class EnsembleHERO(nn.Module):
    """
    é›†æˆå¤šä¸ª HERO æ¨¡å‹çš„é¢„æµ‹ç»“æœ:
    1. ä¸åŒ query_strategy çš„æ¨¡å‹
    2. ä¸åŒæ¸©åº¦å‚æ•°çš„æ¨¡å‹
    3. ä¸åŒè®­ç»ƒ checkpoint çš„æ¨¡å‹
    """
    def __init__(self, models: List[nn.Module], weights=None):
        super().__init__()
        self.models = nn.ModuleList(models)
        self.weights = weights or [1.0] * len(models)
        
    def forward(self, *args, **kwargs):
        outputs = []
        for model in self.models:
            outputs.append(model(*args, **kwargs))
        
        # Weighted average of integrated_context
        ensemble_context = sum(
            w * out.integrated_context 
            for w, out in zip(self.weights, outputs)
        ) / sum(self.weights)
        
        return IntegrationOutput(
            integrated_context=ensemble_context,
            attention_weights=outputs[0].attention_weights,
            global_query=outputs[0].global_query,
            modality_importance=outputs[0].modality_importance,
        )
```

**é¢„æœŸæ”¶ç›Š**: æå‡é¢„æµ‹ç¨³å®šæ€§ï¼Œå°¤å…¶åœ¨è¾¹ç•Œæƒ…å†µä¸‹ã€‚

---

### 2.7 Interpretability Enhancement (å¯è§£é‡Šæ€§å¢å¼º)

**é—®é¢˜**: æ¨¡å‹å†³ç­–è¿‡ç¨‹å¯¹ç”¨æˆ·ä¸é€æ˜ã€‚

**æ–¹æ¡ˆ**:
1. **Attention Visualization**: å°† `modality_importance` æ¸²æŸ“ä¸ºçƒ­åŠ›å›¾
2. **Evidence Highlighting**: åœ¨åŸå§‹è§†é¢‘ä¸­æ ‡æ³¨æ¨¡å‹å…³æ³¨çš„åŒºåŸŸ
3. **Decision Path Logging**: è®°å½• CoT æ¨ç†çš„ä¸­é—´æ­¥éª¤

```python
class InterpretabilityModule:
    def __init__(self, save_dir: str):
        self.save_dir = save_dir
        
    def visualize_attention(self, attention_weights, modality_names, sample_id):
        import matplotlib.pyplot as plt
        
        fig, ax = plt.subplots(figsize=(10, 3))
        ax.bar(modality_names, attention_weights.cpu().numpy())
        ax.set_ylabel('Attention Weight')
        ax.set_title(f'Modality Importance - Sample {sample_id}')
        
        plt.savefig(f'{self.save_dir}/attention_{sample_id}.png')
        plt.close()
        
    def log_cot_reasoning(self, cot_output, sample_id):
        with open(f'{self.save_dir}/reasoning_{sample_id}.json', 'w') as f:
            json.dump(cot_output, f, ensure_ascii=False, indent=2)
```

**é¢„æœŸæ”¶ç›Š**: æå‡æ¨¡å‹å¯ä¿¡åº¦ï¼Œä¾¿äºè°ƒè¯•å’Œè®ºæ–‡å¯è§†åŒ–ã€‚

---

## 3. å®æ–½ä¼˜å…ˆçº§å»ºè®®

| æ–¹æ¡ˆ | å¤æ‚åº¦ | æ”¶ç›Š | å»ºè®®ä¼˜å…ˆçº§ | çŠ¶æ€ |
| :--- | :--- | :--- | :--- | :--- |
| Confidence-Aware Loss | ä½ | ä¸­ | ğŸŸ¡ P1 | âœ… å·²å®ç° |
| Multi-Scale Temporal Fusion | é«˜ | é«˜ | ğŸŸ¢ P2 | â³ å¾…å®ç° |
| MultiModal Contrastive Loss | ä¸­ | é«˜ | ğŸŸ¡ P1 | âœ… å·²å®ç° |
| Modality Entropy Regularizer | ä½ | ä¸­ | ğŸŸ¡ P1 | âœ… å·²å®ç° |
| Emotion-Aware Augmentation | ä¸­ | é«˜ | ğŸŸ¢ P2 | â³ å¾…å®ç° |
| Inference-Time Ensemble | ä½ | ä¸­ | ğŸ”µ P3 | â³ å¾…å®ç° |
| Interpretability Module | ä½ | ä¸­ | ğŸ”µ P3 | âœ… å·²å®ç° |


---

## 4. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1.  **å·²å®Œæˆ**: `MultiModalContrastiveLoss`, `ModalityEntropyRegularizer`, `InterpretabilityModule` å·²å®ç°å¹¶é›†æˆåˆ° `hero_loss.py` å’Œ `interpretability.py`ã€‚
2.  **å·²å®Œæˆ**: å·¥ç¨‹ä¼˜åŒ– (`optimization_utils.py`) åŒ…å« Smart Checkpointing, FlashAttn, QLoRA, AMP, torch.compileã€‚
3.  **ä¸‹ä¸€æ­¥**: åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒéªŒè¯ï¼Œè¯„ä¼°å„ä¼˜åŒ–ç»„ä»¶çš„å®é™…æ”¶ç›Šã€‚

---

*æ›´æ–°æ—¥æœŸ: 2026-01-11*

