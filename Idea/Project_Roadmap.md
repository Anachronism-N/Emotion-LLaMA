# HERO é¡¹ç›®è·¯çº¿å›¾ (Project Roadmap)

> **åŸºäº Emotion-LLaMA çš„ HERO æ¡†æ¶å®ç°**
> é¢å‘å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„åˆ†å±‚å¼è¯æ®æ¨ç†ä¸è§‚å¯Ÿæ¡†æ¶
> (Hierarchical Evidence-based Reasoning and Observation for Multimodal Emotion Understanding)

---

## ğŸ“‹ é¡¹ç›®æ¦‚è§ˆ

### æ ¸å¿ƒç›®æ ‡
å°† Emotion-LLaMA é€æ­¥æ”¹é€ ä¸º HERO æ¡†æ¶ï¼Œå®ç°ï¼š
1. **è§‚æµ‹ä¸“å®¶å±‚ (Observation Expert Layer)** - MoE æ¶æ„çš„å¤šæ¨¡æ€ç‰¹å¾æå–
2. **è¯æ®æ•´åˆå±‚ (Evidence Integration Layer)** - å…¨æ™¯åŠ¨æ€å¼•å¯¼æ³¨æ„åŠ›
3. **åˆ†å±‚æ¨ç†å±‚ (Hierarchical Reasoning Layer)** - ç»“æ„åŒ– CoT æ¨ç†

### å½“å‰è¿›åº¦æ¦‚è§ˆ

| é˜¶æ®µ | çŠ¶æ€ | è¯´æ˜ |
|:-----|:----:|:-----|
| Phase 0: ç¯å¢ƒæ­å»ºä¸åŸºçº¿ç†è§£ | âœ… å®Œæˆ | AU ç‰¹å¾æ¥å…¥å·²å®ç° |
| Phase 1: è§‚æµ‹ä¸“å®¶å±‚æ‰©å±• | âœ… å®Œæˆ | 6 Experts + EvidenceDecoder |
| Phase 2: è¯æ®æ•´åˆå±‚å®ç° | âœ… å®Œæˆ | **AdaptiveQueryGenerator** (3 strategies) |
| Phase 3: åˆ†å±‚æ¨ç†å±‚æ”¹é€  | âœ… å®Œæˆ | CoT Prompt + Structured Output |
| Phase 4: è®­ç»ƒä¸è¯„ä¼° | ğŸŸ¡ å°±ç»ª | Feature Extraction Script Ready |
| **Phase 5: ä¼˜åŒ–ä¸æ‰©å±•** | âœ… å®Œæˆ | è§ä¸‹æ–¹å·²å®ŒæˆåŠŸèƒ½åˆ—è¡¨ |
| **Phase 6: è¿›é˜¶åŠŸèƒ½** | â³ å¾…è§„åˆ’ | Multi-Scale Fusion, Augmentation |

#### Phase 5 å·²å®ŒæˆåŠŸèƒ½æ¸…å•

| åŠŸèƒ½ | æ–‡ä»¶ä½ç½® | è¯´æ˜ |
|:-----|:--------|:-----|
| Evidence Imputation | `evidence_imputation.py` | ç¼ºå¤±æ¨¡æ€ä¼°è®¡ä¸ç½®ä¿¡åº¦è¾“å‡º |
| Temperature Scaling | `integration_layer.py` | å¯å­¦ä¹ æ¸©åº¦å‚æ•° |
| MultiModal Contrastive Loss | `hero_loss.py` | 4 å¯¹æ¨¡æ€å¯¹æ¯”å­¦ä¹  |
| Modality Entropy Regularizer | `hero_loss.py` | é˜²æ­¢å•æ¨¡æ€è¿‡åº¦ä¾èµ– |
| Interpretability Module | `interpretability.py` | å¯è§†åŒ– + CoT æ—¥å¿— |
| Smart Gradient Checkpointing | `optimization_utils.py` | å†»ç»“ Encoder å…¼å®¹ |
| FlashAttention V2 / SDPA | `optimization_utils.py` | é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®— |
| QLoRA Setup | `optimization_utils.py` | 4-bit é‡åŒ– + LoRA |
| Mixed Precision (AMP) | `optimization_utils.py` | BFloat16 è®­ç»ƒ |
| torch.compile | `optimization_utils.py` | PyTorch 2.x ç¼–è¯‘ä¼˜åŒ– |
| **Distributed Training** | `distributed.py` | DDP, FSDP, DeepSpeed |
| **Distributed Inference** | `distributed.py` | å¤š GPU æ¨ç†å¼•æ“ |

---


## ğŸ—ï¸ è¯¦ç»†å®æ–½è®¡åˆ’

### Phase 0: ç¯å¢ƒæ­å»ºä¸åŸºçº¿ç†è§£ âœ…

**å·²å®Œæˆå·¥ä½œ (è¯¦è§ [Implementation_Log.md](./Implementation_Log.md)):**

- [x] 1. æ¥å…¥ AU ç‰¹å¾åˆ° MER2024 æ•°æ®æµ
- [x] 2. è®©æ¨¡å‹æ”¯æŒ 4 è·¯ç‰¹å¾è¾“å…¥ (`feats_llama_proj1-4`)
- [x] 3. é…ç½®é¡¹è¡¥å…… (`au_feature_dir`)
- [x] 4. AU ç‰¹å¾ç¼ºå¤±çš„é²æ£’æ€§å¤„ç†
- [x] 5. AU ç‰¹å¾ç»´åº¦å¯¹é½
- [x] 6. æ¨¡æ€ç¼ºå¤±é²æ£’æ€§ï¼šæ¨¡æ€ä¸¢å¼ƒè®­ç»ƒ

**å½“å‰æ¨¡å‹æ¶æ„ç†è§£:**

```
è¾“å…¥: image, video_features (3-4è·¯)
  â†“
visual_encoder (EVA ViT) â†’ image_embeds â†’ llama_proj â†’ image_inputs_llama
  â†“
video_features â†’ feats_llama_proj[1-4] â†’ video_feats
  â†“
concat(image_inputs_llama, video_feats, cls_tk_feats)
  â†“
LLaMA-2-7B â†’ æƒ…æ„Ÿé¢„æµ‹
```

---

### Phase 1: è§‚æµ‹ä¸“å®¶å±‚æ‰©å±• ğŸ”„

**ç›®æ ‡:** å®ç° HERO çš„åŒè¾“å‡ºä¸“å®¶ç»“æ„ (Feature Tensor + Semantic Evidence)

#### 1.1 Q-Former æ¨¡å—å¼•å…¥ [ä¼˜å…ˆçº§: é«˜]

- [ ] **ä»»åŠ¡ 1.1.1**: ä¸ºè§†è§‰æ¨¡æ€æ·»åŠ  Q-Former
  - å‚è€ƒ BLIP-2/SECap æ¶æ„
  - ä½¿ç”¨å¯å­¦ä¹ æŸ¥è¯¢å‘é‡å‹ç¼©ç‰¹å¾
  - è¾“å‡º: `[B, 32, 768]` çš„ Feature Tensor
  - æ–‡ä»¶ä½ç½®: `minigpt4/models/Qformer.py` (å·²å­˜åœ¨)

- [ ] **ä»»åŠ¡ 1.1.2**: ä¸ºéŸ³é¢‘æ¨¡æ€æ·»åŠ  Q-Former
  - å¤ç”¨ç›¸åŒæ¶æ„ï¼Œç‹¬ç«‹å‚æ•°
  - å¤„ç† HuBERT è¾“å‡ºçš„å˜é•¿åºåˆ—

- [ ] **ä»»åŠ¡ 1.1.3**: Q-Former è¾“å‡ºå¤´è®¾è®¡
  - Head 1: ç‰¹å¾æŠ•å½±å¤´ (Linear â†’ LLM embedding dim)
  - Head 2: è¯æ®è§£ç å¤´ (è½»é‡çº§ Transformer Decoder)

#### 1.2 è¯æ®è§£ç å™¨å®ç° [ä¼˜å…ˆçº§: é«˜]

- [ ] **ä»»åŠ¡ 1.2.1**: å®ç°å…±äº«çš„ Evidence Decoder
  - ä½¿ç”¨ Task Token åŒºåˆ†æ¨¡æ€: `<visual_task>`, `<audio_task>`
  - è¾“å‡ºè‡ªç„¶è¯­è¨€æè¿°çš„è¯­ä¹‰è¯æ®
  - ç¤ºä¾‹: "è§†è§‰è¯æ® (VE-01): è§‚å¯Ÿåˆ°äººç‰©é¢éƒ¨å‡ºç°å¾®ç¬‘è¡¨æƒ… (AU12)"

- [ ] **ä»»åŠ¡ 1.2.2**: è¯æ®æ¨¡æ¿è®¾è®¡
  - è®¾è®¡æ ‡å‡†åŒ–çš„è¯æ®è¾“å‡ºæ ¼å¼
  - åŒ…å« AU ç¼–å·ã€ç½®ä¿¡åº¦ç­‰ç»“æ„åŒ–ä¿¡æ¯

#### 1.3 Synergy Expert (ååŒä¸“å®¶) [ä¼˜å…ˆçº§: ä¸­]

- [ ] **ä»»åŠ¡ 1.3.1**: å®ç°éŸ³ç”»ååŒæ„ŸçŸ¥æ¨¡å—
  - æ£€æµ‹éŸ³ç”»åŒæ­¥æ€§/å†²çª
  - ç”¨äºè¯†åˆ«åè®½ã€è‹¦ç¬‘ç­‰æƒ…å†µ
  - è¾“å‡º: åŒæ­¥æ€§åˆ†æ•° + å†²çªæ ‡è®°

**æ¶‰åŠæ–‡ä»¶:**
- `minigpt4/models/minigpt_v2.py` - ä¸»è¦ä¿®æ”¹
- `minigpt4/models/observation_experts.py` - **æ–°å»º**
- `minigpt4/models/evidence_decoder.py` - **æ–°å»º**

---

### Phase 2: è¯æ®æ•´åˆå±‚å®ç° â³

**ç›®æ ‡:** å®ç°å…¨æ™¯åŠ¨æ€å¼•å¯¼æ³¨æ„åŠ› (Panoramic Dynamic Guided Attention)

#### 2.1 å…¨å±€æŸ¥è¯¢ç”Ÿæˆ [ä¼˜å…ˆçº§: é«˜]

- [ ] **ä»»åŠ¡ 2.1.1**: å®ç° Summary å‘é‡èšåˆ
  - æ”¶é›†å„ä¸“å®¶çš„ `[CLS]` Token æˆ– Global Average Pooling ç»“æœ
  - åŒ…å«: `v_vis_global`, `v_vis_motion`, `v_vis_au`, `v_audio`, `v_text`, `v_synergy`

- [ ] **ä»»åŠ¡ 2.1.2**: ç”Ÿæˆå…¨å±€æŸ¥è¯¢å‘é‡ $Q_{global}$
  - è½»é‡çº§ Transformer Encoder èåˆæ‰€æœ‰ summary å‘é‡
  - è¾“å‡º: åŒ…å«å…¨æ¨¡æ€ä¿¡æ¯çš„ç»Ÿä¸€æŸ¥è¯¢

#### 2.2 K-Bank æ„å»ºä¸æ³¨æ„åŠ›è®¡ç®— [ä¼˜å…ˆçº§: é«˜]

- [ ] **ä»»åŠ¡ 2.2.1**: æ„å»ºç»†èŠ‚ç‰¹å¾åº“ K-Bank
  - æ‹¼æ¥æ‰€æœ‰å•æ¨¡æ€ä¸“å®¶çš„å®Œæ•´ç‰¹å¾åºåˆ—
  - å½¢çŠ¶: `[B, Total_Seq_Len, Dim]`

- [ ] **ä»»åŠ¡ 2.2.2**: å®ç° Panoramic-Guided Attention
  - ä½¿ç”¨ $Q_{global}$ æ£€ç´¢ K-Bank ä¸­çš„å…³é”®ä¿¡æ¯
  - ç”Ÿæˆ: `Integrated_Context_Tensor` + `Dynamic_Attention_Weights`

#### 2.3 æ¨¡æ€ç¼ºå¤±é²æ£’æ€§æœºåˆ¶ [ä¼˜å…ˆçº§: ä¸­]

- [ ] **ä»»åŠ¡ 2.3.1**: è®­ç»ƒæ—¶ - éšå¼è¡¨å¾å¯¹é½
  - å®ç° Teacher-Student æ¶æ„
  - å¤šæ¨¡æ€èåˆä¸“å®¶ä½œä¸º"æ•™å¸ˆ"
  - ä½¿ç”¨ Modality Dropout + KL æ•£åº¦æŸå¤±

- [ ] **ä»»åŠ¡ 2.3.2**: æ¨ç†æ—¶ - æ˜¾å¼è¯æ®è¡¥å…¨
  - è½»é‡çº§è¯æ®è¡¥å…¨æ¨¡å—
  - åŸºäºå¯ç”¨è¯æ®æ¨æ–­ç¼ºå¤±è¯æ®

**æ¶‰åŠæ–‡ä»¶:**
- `minigpt4/models/integration_layer.py` - **æ–°å»º**
- `minigpt4/models/minigpt_v2.py` - è°ƒç”¨æ•´åˆå±‚

---

### Phase 3: åˆ†å±‚æ¨ç†å±‚æ”¹é€  â³

**ç›®æ ‡:** å®ç°ç»“æ„åŒ– CoT æ¨ç†ä¸ JSON è¾“å‡º

#### 3.1 æ··åˆå¼è¾“å…¥åµŒå…¥ [ä¼˜å…ˆçº§: é«˜]

- [ ] **ä»»åŠ¡ 3.1.1**: ç‰¹å¾æŠ•å½±ä¸æ³¨å…¥
  - Linear Projector: `Integrated_Context_Tensor` â†’ LLM embedding
  - ä½œä¸º `<visual_audio_token>` æ’å…¥ Prompt å¼€å¤´

- [ ] **ä»»åŠ¡ 3.1.2**: æ–‡æœ¬è¯æ®æ‹¼æ¥
  - æŒ‰æ³¨æ„åŠ›æƒé‡æ’åºè¯­ä¹‰è¯æ®
  - æ‹¼æ¥åœ¨ç‰¹å¾ Token ä¹‹å

#### 3.2 ç»“æ„åŒ–æŒ‡ä»¤å¾®è°ƒ [ä¼˜å…ˆçº§: é«˜]

- [ ] **ä»»åŠ¡ 3.2.1**: è®¾è®¡ JSON è¾“å‡ºæ ¼å¼
  ```json
  {
    "emotion_caption": "æƒ…æ„Ÿæè¿°",
    "evidence_summary": ["è¯æ®1", "è¯æ®2"],
    "reasoning_process": "æ¨ç†é€»è¾‘",
    "final_emotion": "æƒ…æ„Ÿæ ‡ç­¾"
  }
  ```

- [ ] **ä»»åŠ¡ 3.2.2**: æ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®
  - æ‰©å±• MERR æ•°æ®é›†
  - æ·»åŠ  CoT æ¨ç†æ ‡æ³¨

#### 3.3 LLM åŸºåº§å¯é€‰æ›¿æ¢ [ä¼˜å…ˆçº§: ä½]

- [ ] **ä»»åŠ¡ 3.3.1**: æ”¯æŒ Qwen-2-7B-Instruct (å¯é€‰)
  - æ›¿æ¢ LLaMA-2-7B
  - è°ƒæ•´ Tokenizer å’Œç”Ÿæˆé…ç½®

**æ¶‰åŠæ–‡ä»¶:**
- `minigpt4/models/minigpt_base.py` - ä¿®æ”¹ `preparing_embedding`, `forward`
- `minigpt4/conversation/conversation.py` - Prompt æ¨¡æ¿

---

### Phase 4: è®­ç»ƒä¸è¯„ä¼° â³

**ç›®æ ‡:** å®ç°ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ

#### 4.1 Stage 1: æ¨¡æ€è§£çº ç¼ ä¸è¡¨å¾å¯¹é½

- [ ] **ä»»åŠ¡ 4.1.1**: å®ç° ITC æŸå¤± (Image/Audio-Text Contrastive)
- [ ] **ä»»åŠ¡ 4.1.2**: å®ç° STMIL æŸå¤± (Speech-Text Mutual Information Learning)
- [ ] **ä»»åŠ¡ 4.1.3**: å®ç° Synergy é¢„è®­ç»ƒæŸå¤±

```math
\mathcal{L}_{Stage1} = \mathcal{L}_{ITC} + \lambda_1 \mathcal{L}_{STMIL} + \lambda_2 \mathcal{L}_{Synergy}
```

#### 4.2 Stage 2: ç”Ÿæˆå¼æƒ…æ„Ÿé¢„è®­ç»ƒ

- [ ] **ä»»åŠ¡ 4.2.1**: å®ç° Caption Generation æŸå¤±
- [ ] **ä»»åŠ¡ 4.2.2**: å®ç° SCCL æŸå¤± (Speech-Caption Contrastive)
- [ ] **ä»»åŠ¡ 4.2.3**: å®ç° KL æ•£åº¦æŸå¤± (é²æ£’æ€§)

```math
\mathcal{L}_{Stage2} = \mathcal{L}_{Gen} + \lambda_3 \mathcal{L}_{SCCL} + \lambda_4 \mathcal{L}_{KL}
```

#### 4.3 Stage 3: å…¨ç›‘ç£æŒ‡ä»¤å¾®è°ƒ

- [ ] **ä»»åŠ¡ 4.3.1**: ç»“æ„åŒ– CoT æ¨ç†è®­ç»ƒ
- [ ] **ä»»åŠ¡ 4.3.2**: åäº‹å®æ ·æœ¬è®­ç»ƒ (éŸ³ç”»å†²çª)

```math
\mathcal{L}_{Stage3} = \mathcal{L}_{Struct\_Gen}
```

#### 4.4 è¯„ä¼°æŒ‡æ ‡å®ç°

- [ ] **ä»»åŠ¡ 4.4.1**: æ€§èƒ½æŒ‡æ ‡ (WAF, Accuracy, F1)
- [ ] **ä»»åŠ¡ 4.4.2**: é²æ£’æ€§æŒ‡æ ‡ (Noise Drop Rate, Sarcasm Detection)
- [ ] **ä»»åŠ¡ 4.4.3**: ç”Ÿæˆè´¨é‡æŒ‡æ ‡ (CIDEr, SPICE, LLM-as-a-Judge)

**æ¶‰åŠæ–‡ä»¶:**
- `minigpt4/common/hero_losses.py` - **æ–°å»º**
- `minigpt4/runners/runner_base.py` - è®­ç»ƒé€»è¾‘ä¿®æ”¹
- `eval_hero.py` - **æ–°å»º**

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„ (é¢„æœŸ)

```
Emotion-LLaMA/
â”œâ”€â”€ minigpt4/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ minigpt_v2.py          # [ä¿®æ”¹] ä¸»æ¨¡å‹å…¥å£
â”‚   â”‚   â”œâ”€â”€ observation_experts.py # [æ–°å»º] è§‚æµ‹ä¸“å®¶å±‚
â”‚   â”‚   â”œâ”€â”€ evidence_decoder.py    # [æ–°å»º] è¯æ®è§£ç å™¨
â”‚   â”‚   â”œâ”€â”€ integration_layer.py   # [æ–°å»º] è¯æ®æ•´åˆå±‚
â”‚   â”‚   â”œâ”€â”€ hero_model.py          # [æ–°å»º] HERO ä¸»æ¨¡å‹
â”‚   â”‚   â””â”€â”€ Qformer.py             # [ç°æœ‰] Q-Former æ¨¡å—
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â””â”€â”€ hero_losses.py         # [æ–°å»º] æŸå¤±å‡½æ•°
â”‚   â””â”€â”€ datasets/
â”‚       â””â”€â”€ datasets/
â”‚           â””â”€â”€ mer2024.py         # [ä¿®æ”¹] æ•°æ®åŠ è½½
â”œâ”€â”€ Idea/
â”‚   â”œâ”€â”€ Idea.md                    # åŸå§‹ idea æ–‡æ¡£
â”‚   â”œâ”€â”€ Project_Roadmap.md         # æœ¬æ–‡æ¡£
â”‚   â””â”€â”€ Implementation_Log.md      # å®ç°è®°å½•
â”œâ”€â”€ train_configs/
â”‚   â””â”€â”€ hero_*.yaml                # [æ–°å»º] HERO è®­ç»ƒé…ç½®
â””â”€â”€ eval_hero.py                   # [æ–°å»º] HERO è¯„ä¼°è„šæœ¬
```

---

## â±ï¸ æ—¶é—´çº¿ä¼°ç®—

| é˜¶æ®µ | é¢„è®¡æ—¶é—´ | é‡Œç¨‹ç¢‘ |
|:-----|:--------:|:-------|
| Phase 1 | 2-3 å‘¨ | Q-Former + Evidence Decoder å®Œæˆ |
| Phase 2 | 2 å‘¨ | è¯æ®æ•´åˆå±‚å®Œæˆ |
| Phase 3 | 1-2 å‘¨ | ç»“æ„åŒ–æ¨ç†æ”¹é€ å®Œæˆ |
| Phase 4 | 3-4 å‘¨ | ä¸‰é˜¶æ®µè®­ç»ƒ + è¯„ä¼° |
| **æ€»è®¡** | **8-11 å‘¨** | HERO v1.0 å®Œæˆ |

---

## ğŸ”— ç›¸å…³èµ„æº

- **Idea åŸæ–‡**: [Idea.md](./Idea.md)
- **å®ç°è®°å½•**: [Implementation_Log.md](./Implementation_Log.md)
- **Emotion-LLaMA è®ºæ–‡**: [arXiv](https://arxiv.org/pdf/2406.11161)
- **BLIP-2 è®ºæ–‡**: [arXiv](https://arxiv.org/abs/2301.12597)
- **MER2024 Challenge**: [å®˜æ–¹ç½‘ç«™](http://merchallenge.cn/)

---

*æœ€åæ›´æ–°: 2026-01-10*

---

## ğŸš€ Phase 5: ä¼˜åŒ–ä¸æ‰©å±•è®¡åˆ’ (æœªå®ç°åŠŸèƒ½ & æ½œåœ¨æ”¹è¿›)

æœ¬èŠ‚åŸºäº `Idea.md` ä¸å½“å‰å®ç°çš„å¯¹é½æ£€æŸ¥ç»“æœï¼Œåˆ—å‡ºå°šæœªå®ç°çš„åŠŸèƒ½ä»¥åŠå¯ä¼˜åŒ–æ–¹å‘ã€‚

### 5.1 æœªå®ç°åŠŸèƒ½ (Gap Analysis)

| åŠŸèƒ½ | Idea.md ä½ç½® | å½“å‰çŠ¶æ€ | ä¼˜å…ˆçº§ |
| :--- | :--- | :--- | :--- |
| **æ˜¾å¼è¯æ®è¡¥å…¨ (Evidence Imputation)** | Line 188 | æœªå®ç° | ğŸ”´ é«˜ |
| **EvidenceDecoder çš„ç»†ç²’åº¦æ–‡æœ¬è®­ç»ƒæ•°æ®** | Pillar 1 | ç¼ºå°‘ç›‘ç£ä¿¡å· | ğŸ”´ é«˜ |
| **OpenFace AU ç‰¹å¾å®æ—¶é›†æˆ** | Pillar 1 | Placeholder (Zeros) | ğŸŸ¡ ä¸­ |
| **LLM æƒ…æ„Ÿè¯æ±‡æ‰©å±• (Tokenizer)** | Pillar 3 | ä½¿ç”¨é»˜è®¤ LLaMA | ğŸŸ¢ ä½ |

### 5.2 å¯ä¼˜åŒ–æ–¹å‘ (Optimization Proposals)

#### A. åŠ¨æ€é˜ˆå€¼æ›´æ–° (Dynamic Threshold for Scorer)
*   **ç°çŠ¶**: `AdaptiveQueryGenerator (dynamic)` ä¸­çš„ Scorer è¾“å‡ºçš„æ˜¯ç»å¯¹åˆ†æ•°ã€‚
*   **ä¼˜åŒ–**: å¼•å…¥ **Temperature Scaling** (å¦‚ `softmax(scores / T)`) æˆ–è€… **Top-K Gating**ï¼Œå…è®¸æ¨¡å‹åªå…³æ³¨å‰ K ä¸ªæœ€ä¿¡ä»»çš„æ¨¡æ€ã€‚
*   **é¢„æœŸæ”¶ç›Š**: æå‡åœ¨æç«¯å™ªå£°åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚

#### B. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing for Memory)
*   **ç°çŠ¶**: 6 ä¸ª Q-Former + LLM å¯èƒ½å¯¼è‡´æ˜¾å­˜æ‰“æ»¡ã€‚
*   **ä¼˜åŒ–**: åœ¨ `ModalityQFormer` å’Œ `HEROModel` ä¸­å¯ç”¨ `torch.utils.checkpoint.checkpoint` æ¥æ¢æ—¶é—´ä¸å†…å­˜ã€‚
*   **é¢„æœŸæ”¶ç›Š**: Batch Size å¯ä»¥å¢åŠ  2-3 å€ã€‚

#### C. LoRA å¾®è°ƒé›†æˆ
*   **ç°çŠ¶**: LLM å…¨å‚æ•°è®­ç»ƒã€‚
*   **ä¼˜åŒ–**: é›†æˆ PEFT åº“ï¼Œå¯¹ LLM çš„ QKV å±‚åº”ç”¨ LoRAã€‚
*   **é¢„æœŸæ”¶ç›Š**: è®­ç»ƒæ—¶é—´ç¼©çŸ­ 40% ä»¥ä¸Šï¼Œæ˜¾å­˜å ç”¨é™ä½ã€‚

#### D. è¯æ®è¡¥å…¨æ¨¡å— (Implement Evidence Imputation)
*   **ç°çŠ¶**: æœªå®ç°ã€‚
*   **å®ç°æ–¹æ¡ˆ**:
    1.  è®­ç»ƒä¸€ä¸ªè½»é‡çº§ Transformer Decoderï¼Œä»¥å…¶ä»–æ¨¡æ€çš„ Summary ä¸ºè¾“å…¥ã€‚
    2.  è¾“å‡º: ç¼ºå¤±æ¨¡æ€çš„ä¼°è®¡ Summary å‘é‡ã€‚
    3.  å¯è¿˜å¯ä»¥è¾“å‡ºè¯´æ˜æ–‡æœ¬ï¼Œå¦‚ `[IAE-01]: æ¨æ–­éŸ³é¢‘æƒ…æ„Ÿä¸Šæ‰¬`ã€‚
*   **è®­ç»ƒæ•°æ®**: ä½¿ç”¨ Modality Dropout ç”Ÿæˆçš„ (Teacher-Output, Dropped-Input) å¯¹è¿›è¡Œç›‘ç£ã€‚

---
